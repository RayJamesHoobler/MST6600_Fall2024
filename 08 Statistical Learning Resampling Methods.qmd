---
title: "Resampling Methods" 
subtitle: "Module 08"
author: "Ray J. Hoobler"
bibliography: references.bib
execute:
  echo: true
  cache: false # requires a specific page
title-slide-attributes:
  data-background-color: "#1178c9"
format: 
  revealjs:
    df-print: paged
    toc: true
    toc-depth: 1
    toc-title: "Table of Contents"
    embed-resources: true
    link-external-icon: true
    link-external-newwindow: true
    incremental: false
    smaller: false
    scrollable: true
    transition: fade
    code-fold: true
    code-tools: true
    show-slide-number: all
    slide-number: c/t
    footer: "Applied Statistical Techniques"
    theme: [simple, mysimple.scss]
---

## Libraries

```{r}
#| code-fold: show 
library(tidyverse)
library(broom)
library(patchwork)
```

# Cross-Validation {.theme-section}

## The Validation Set Approach  

::: r-stack
```{=html}
<svg xmlns="http://www.w3.org/2000/svg" width="600" height="600" viewBox="0 80 320 150">
  <!-- Outer box -->
  <rect x="8" y="8" width="304" height="110" fill="#f0f0f0" stroke="#000000" stroke-width="1.6"/>
  
  <!-- Training Set (Blue) -->
  <rect x="16" y="16" width="140" height="94" fill="#b3d9ff" stroke="#000000" stroke-width="0.8"/>
  <text x="20" y="30" font-family="Arial" font-size="11.2" fill="#000000">Training Set (50%)</text>
  <text x="20" y="50" font-family="Arial" font-size="9.6" fill="#000000">Observations:</text>
  <text x="20" y="66" font-family="Arial" font-size="9.6" fill="#000000">7, 22, 13, ...</text>
  
  <!-- Validation Set (Beige) -->
  <rect x="164" y="16" width="140" height="94" fill="#f5e6c9" stroke="#000000" stroke-width="0.8"/>
  <text x="168" y="30" font-family="Arial" font-size="11.2" fill="#000000">Validation Set (50%)</text>
  <text x="168" y="50" font-family="Arial" font-size="9.6" fill="#000000">Observations:</text>
  <text x="168" y="66" font-family="Arial" font-size="9.6" fill="#000000">91, ...</text>
  
  <!-- Arrow -->
  <line x1="160" y1="8" x2="160" y2="122" stroke="#000000" stroke-width="1.6" stroke-dasharray="4,4"/>
  <polygon points="156,122 160,130 164,122" fill="#000000"/>
  
  <!-- Title -->
  <text x="160" y="144" font-family="Arial" font-size="12.8" fill="#000000" text-anchor="middle">Random 50% Split</text>
</svg>
```
:::


## The Validation Set Approach {.smaller}

### Auto MPG dataset [@auto_mpg_9]

-   398 observations
-   9 variables
-   Predict `mpg` using `horsepower`

```{r}
#| code-fold: show 
#| code-line-numbers: "|1|3-5|10-11|"

auto_mpg_column_names <- c("mpg", "cylinders", "displacement", "horsepower", "weight", "acceleration", "model_year", "origin", "car_name")

auto_mpg_spaces <- read_table("datasets/auto+mpg/auto-mpg.data", na = "?", col_names = auto_mpg_column_names[1:8]) 

auto_mpg_tab <- read_delim("datasets/auto+mpg/auto-mpg.data", delim = "\t", col_names = FALSE) |> select(2)

# auto_mpg_spaces
# auto_mpg_tab

auto_mpg_9 <- bind_cols(auto_mpg_spaces, auto_mpg_tab) |> 
  set_names(auto_mpg_column_names)

summary(auto_mpg_9)
```

```{r}
# there are 6 NA values in the horsepower column 

auto_mpg_9_final <- auto_mpg_9 |> 
  drop_na(horsepower)

summary(auto_mpg_9_final)
```

## Plot of the data

```{r}
#| code-fold: true
#| fig-width: 6
#| out-width: 600px

auto_mpg_9_final |> 
  ggplot(aes(x = horsepower, y = mpg)) +
  geom_point() +
  labs(
    title = "Auto MPG Dataset",
    x = "Horsepower",
    y = "MPG"
  ) +
  labs(
    title = "Exploratory Data Analysis (EDA) of Auto MPG dataset",
    subtitle = "Understanding the relationship between Horsepower and MPG",
    x = "Horsepower",
    y = "MPG",
    caption = "Source: Auto MPG dataset from UCI Machine Learning Repository"
  ) +
  theme_light()
```

## Example: Validation Set Approach (1/20)

### Random 50% test train split of the data

```{r}
#| code-fold: show

set.seed(42)
# Create a random 50% split of the data index using the response variable mpg

auto_mpg_train <- caret::createDataPartition(auto_mpg_9_final$mpg, p = 0.5, list = FALSE)
head(auto_mpg_train, 10)
```

## Example: Validation Set Approach (2/20)

### Random 50% test train split of the data

```{r}
#| code-fold: show

auto_mpg_train_data <- auto_mpg_9_final[auto_mpg_train,]
auto_mpg_test_data <- auto_mpg_9_final[-auto_mpg_train,]

auto_mpg_train_data
auto_mpg_test_data
```

## Example: Validation Set Approach (3/20)

### Fit a linear model on the training data

```{r}
#| code-fold: show

auto_mpg_lm <- lm(mpg ~ horsepower, data = auto_mpg_train_data)
summary(auto_mpg_lm)
```

## Example: Validation Set Approach (4/20)

### Predict the test data

```{r}
#| code-fold: show

auto_mpg_test_pred <- predict(auto_mpg_lm, newdata = auto_mpg_test_data)
auto_mpg_test_pred
```

## Example: Validation Set Approach (5/20)

### Calculate the MSE

```{r}
#| code-fold: show

mpg_mse_lm <- mean((auto_mpg_test_data$mpg - auto_mpg_test_pred)^2)
mpg_mse_lm
```

## Example: Validation Set Approach (6/20)

### Repeat the process for a quadratic model

```{r}
#| code-fold: show

auto_mpg_lm_quad <- lm(mpg ~ horsepower + I(horsepower^2), data = auto_mpg_train_data)
summary(auto_mpg_lm_quad)
```

## Example: Validation Set Approach (7/20)

### Predict the test data of the quadratic model

```{r}
#| code-fold: show

auto_mpg_test_pred_quad <- predict(auto_mpg_lm_quad, newdata = auto_mpg_test_data)
auto_mpg_test_pred_quad
```

## Example: Validation Set Approach (8/20)

### Calculate the MSE of the quadratic model

```{r}
#| code-fold: show 

mpg_mse_quad <- mean((auto_mpg_test_data$mpg - auto_mpg_test_pred_quad)^2)
mpg_mse_quad
```

## Example: Validation Set Approach (9/20)

### Repeat the process for a cubic model

```{r}
#| code-fold: show

auto_mpg_lm_cubic <- lm(mpg ~ horsepower + I(horsepower^2) + I(horsepower^3), data = auto_mpg_train_data)
summary(auto_mpg_lm_cubic)
```

## Example: Validation Set Approach (10/20)

### Predict the test data of the cubic model

```{r}
#| code-fold: show

auto_mpg_test_pred_cubic <- predict(auto_mpg_lm_cubic, newdata = auto_mpg_test_data)
auto_mpg_test_pred_cubic
```

## Example: Validation Set Approach (11/20)

### Calculate the MSE of the cubic model

```{r}
#| code-fold: show

mpg_mse_cubic <- mean((auto_mpg_test_data$mpg - auto_mpg_test_pred_cubic)^2)
mpg_mse_cubic
```

## Example: Validation Set Approach (12/20)

### Compare the MSE of the models

```{r}
#| code-fold: show

# Create a data frame of the MSE values
mpg_mse_df <- tibble(
  Model = c("Linear", "Quadratic", "Cubic"),
  MSE = round(c(mpg_mse_lm, mpg_mse_quad, mpg_mse_cubic), 2)
)

mpg_mse_df |> 
  gt::gt()
```

# DRY (Don't Repeat Yourself) {.theme-section}

## Example: Validation Set Approach (13/20)

### Use the poly() function and use a for loop

```{r}
#| code-fold: show
#| code-line-numbers: "|2-3|5-10|12-15|"

# Create a for loop to fit the models and calculate the MSE
mpg_model_order <- c()
mpg_mse <- c()

for (i in 1:6) {
  auto_mpg_lm <- lm(mpg ~ poly(horsepower, i), data = auto_mpg_train_data)
  auto_mpg_test_pred <- predict(auto_mpg_lm, newdata = auto_mpg_test_data)
  mpg_model_order[i] <- i
  mpg_mse[i] <- mean((auto_mpg_test_data$mpg - auto_mpg_test_pred)^2)
}

mpg_model_dry_df <- tibble(
  model_order = mpg_model_order,
  mpg_mse
)

mpg_model_dry_df
```

## Example: Validation Set Approach (14/20

### Plot the MSE values

```{r}
#| code-fold: true

mpg_model_dry_df |> 
  ggplot(aes(x = model_order, y = mpg_mse)) +
  geom_point() +
  geom_line() +
  labs(
    title = "MSE vs. Model Order",
    x = "Model Order",
    y = "MSE"
  ) +
  labs(
    title = "Validation Set Approach to Model Validaition",
    subtitle = "Data split 50/50",
    x = "Model Order",
    y = "MSE",
    caption = "Source: Auto MPG dataset from UCI Machine Learning Repository"
  ) +
  scale_x_continuous(breaks = 1:6) +
  theme_light()
```

## Example: Validation Set Approach (15/20)

### How does MSE vary with our choice of the "random seed" used to split the data?

```{r}
#| code-fold: show

mse_seed <- function(seed_values) {
  mse_values <- numeric(length(seed_values))
  
  for (i in seq_along(seed_values)) {
    set.seed(seed_values[i])
    
    auto_mpg_train <- caret::createDataPartition(auto_mpg_9_final$mpg, p = 0.5, list = FALSE)
    auto_mpg_train_data <- auto_mpg_9_final[auto_mpg_train,]
    auto_mpg_test_data <- auto_mpg_9_final[-auto_mpg_train,]
    
    auto_mpg_lm <- lm(mpg ~ horsepower, data = auto_mpg_train_data)
    auto_mpg_test_pred <- predict(auto_mpg_lm, newdata = auto_mpg_test_data)
    
    mse_values[i] <- mean((auto_mpg_test_data$mpg - auto_mpg_test_pred)^2)
  }
  
  result <- tibble::tibble(Seed = seed_values, MSE = round(mse_values, 3))
  return(result)
}

# Example usage:
seed_values <- c(42, sample(1:9999, 9))
result <- mse_seed(seed_values)
result
```

## Example: Validation Set Approach (16/20)

::: columns
::: {.column width="50%"}
### Calculate the MSE values for different seeds and model orders

```{r}
mse_seed_poly <- function(seed_values, max_degree = 3) {
  results <- tibble()
  
  for (seed in seed_values) {
    set.seed(seed)
    
    auto_mpg_train <- caret::createDataPartition(auto_mpg_9_final$mpg, p = 0.5, list = FALSE)
    auto_mpg_train_data <- auto_mpg_9_final[auto_mpg_train,]
    auto_mpg_test_data <- auto_mpg_9_final[-auto_mpg_train,]
    
    for (degree in 1:max_degree) {
      formula <- as.formula(paste("mpg ~ poly(horsepower, degree =", degree, ")"))
      auto_mpg_lm <- lm(formula, data = auto_mpg_train_data)
      auto_mpg_test_pred <- predict(auto_mpg_lm, newdata = auto_mpg_test_data)
      
      mse <- mean((auto_mpg_test_data$mpg - auto_mpg_test_pred)^2)
      
      results <- rbind(results, data.frame(Seed = seed, Degree = degree, MSE = mse))
    }
  }
  
  return(results)
}

# Example usage:
set.seed(42)  # Set a seed for reproducibility
seed_values <- c(42, sample(1:9999, 10))
result_df <- mse_seed_poly(seed_values, max_degree = 6)

result_df
```
:::

::: {.column width="50%"}
### Plot the MSE values for different seeds

```{r}
#| code-fold: true
#| fig-width: 6
#| out-width: 600px

result_df |> 
  ggplot(aes(x = Degree, y = MSE, color = factor(Seed))) +
  geom_line(show.legend = FALSE) +
  labs(
    title = "MSE vs. Model Order",
    x = "Model Order",
    y = "MSE"
  ) +
  labs(
    title = "Validation Set Approach to Model Validaition",
    subtitle = "Data split 50/50; 11 random seeds",
    x = "Model Order",
    y = "MSE",
    caption = "Source: Auto MPG dataset from UCI Machine Learning Repository"
  ) +
  scale_x_continuous(breaks = 1:6) +
  theme_light()
```
:::
:::

## Example: Validation Set Approach (17/20)

### Mean values for the MSE

```{r}
#| code-fold: show

result_df |> 
  group_by(Degree) |> 
  summarise(MSE = mean(MSE))
```

## Example: Validation Set Approach (18/20)

### Plot the predicted values versus the actual values for order 2

```{r}
#| code-fold: true

auto_mpg_lm <- lm(mpg ~ poly(horsepower, 2), data = auto_mpg_train_data)
auto_mpg_test_pred <- predict(auto_mpg_lm, newdata = auto_mpg_test_data)

auto_mpg_test_data |> 
  ggplot(aes(x = horsepower, y = mpg)) +
  geom_point() +
  geom_line(aes(y = auto_mpg_test_pred), color = "red") +
  labs(
    title = "Predicted vs. Actual Values",
    x = "Horsepower",
    y = "MPG"
  ) +
  labs(
    title = "Validation Set Approach",
    subtitle = "Data split 50/50; Model Order 2",
    x = "Horsepower",
    y = "MPG",
    caption = "Source: Auto MPG dataset from UCI Machine Learning Repository"
  ) +
  theme_light()
```

## Example: Validation Set Approach (19/20)

### What about a Poisson model?

```{r}
#| code-fold: show

# auto_mpg_poisson <- glm(mpg ~ horsepower, data = auto_mpg_train_data, family = "poisson")
# auto_mpg_test_pred_poisson <- predict(auto_mpg_poisson, newdata = auto_mpg_test_data, type = "response")

auto_mpg_gamma <- glm(mpg ~ horsepower, data = auto_mpg_train_data, family = Gamma(link = "log"))
auto_mpg_test_pred_gamma <- predict(auto_mpg_gamma, newdata = auto_mpg_test_data, type = "response")

# mpg_mse_poisson <- mean((auto_mpg_test_data$mpg - auto_mpg_test_pred_poisson)^2)
# mpg_mse_poisson

mpg_mse_gamma <- mean((auto_mpg_test_data$mpg - auto_mpg_test_pred_gamma)^2)
mpg_mse_gamma
```

## Example: Validation Set Approach (20/20)

### Plot the predicted values versus the actual values for the Poisson and gamma models

```{r}
#| code-fold: true

auto_mpg_test_data |> 
  ggplot(aes(x = horsepower, y = mpg)) +
  geom_point() +
  geom_line(aes(y = auto_mpg_test_pred_gamma), color = "blue") +
  labs(
    title = "Predicted vs. Actual Values",
    x = "Horsepower",
    y = "MPG"
  ) +
  labs(
    title = "Validation Set Approach to Model Validaition",
    subtitle = "Data split 50/50; Gamma (blue line) model",
    x = "Horsepower",
    y = "MPG",
    caption = "Source: Auto MPG dataset from UCI Machine Learning Repository"
  ) +
  theme_light()
```

## Leave-One-Out Cross Validation (LOOCV)

## 

::: r-stack
```{=html}
<svg xmlns="http://www.w3.org/2000/svg" width="600" height="600" viewBox="0 0 225 225">
  <!-- Title -->
  <text x="112.5" y="12.5" font-family="Arial" font-size="9" fill="#000000" text-anchor="middle" font-weight="bold">Leave-One-Out Cross Validation (LOOCV)</text>

  <!-- All Data -->
  <rect x="62.5" y="20" width="100" height="15" fill="#b3d9ff" stroke="#000000" stroke-width="0.5"/>
  <text x="112.5" y="30" font-family="Arial" font-size="6" fill="#000000" text-anchor="middle">All Data (n observations)</text>
  
  <!-- Arrow -->
  <line x1="112.5" y1="37.5" x2="112.5" y2="47.5" stroke="#000000" stroke-width="1"/>
  <polygon points="110,47.5 112.5,52.5 115,47.5" fill="#000000"/>
  
  <!-- LOOCV Iterations -->
  <g transform="translate(37.5,55)">
    <!-- Iteration 1 -->
    <rect x="25" y="0" width="100" height="12.5" fill="#b3d9ff" stroke="#000000" stroke-width="0.5"/>
    <rect x="25" y="0" width="10" height="12.5" fill="#f5e6c9" stroke="#000000" stroke-width="0.5"/>
    <text x="30" y="8.5" font-family="Arial" font-size="6" fill="#000000" text-anchor="middle">1</text>
    <text x="120" y="8.5" font-family="Arial" font-size="6" fill="#000000" text-anchor="end">Iteration 1</text>
    
    <!-- Iteration 2 -->
    <rect x="25" y="15" width="100" height="12.5" fill="#b3d9ff" stroke="#000000" stroke-width="0.5"/>
    <rect x="35" y="15" width="10" height="12.5" fill="#f5e6c9" stroke="#000000" stroke-width="0.5"/>
    <text x="40" y="23.5" font-family="Arial" font-size="6" fill="#000000" text-anchor="middle">2</text>
    <text x="120" y="23.5" font-family="Arial" font-size="6" fill="#000000" text-anchor="end">Iteration 2</text>
    
    <!-- Iteration 3 -->
    <rect x="25" y="30" width="100" height="12.5" fill="#b3d9ff" stroke="#000000" stroke-width="0.5"/>
    <rect x="45" y="30" width="10" height="12.5" fill="#f5e6c9" stroke="#000000" stroke-width="0.5"/>
    <text x="50" y="38.5" font-family="Arial" font-size="6" fill="#000000" text-anchor="middle">3</text>
    <text x="120" y="38.5" font-family="Arial" font-size="6" fill="#000000" text-anchor="end">Iteration 3</text>
    
    <!-- Ellipsis to indicate continuation -->
    <text x="75" y="52.5" font-family="Arial" font-size="10" fill="#000000" text-anchor="middle">...</text>
    
    <!-- Last Iteration -->
    <rect x="25" y="62.5" width="100" height="12.5" fill="#b3d9ff" stroke="#000000" stroke-width="0.5"/>
    <rect x="115" y="62.5" width="10" height="12.5" fill="#f5e6c9" stroke="#000000" stroke-width="0.5"/>
    <text x="120" y="71" font-family="Arial" font-size="6" fill="#000000" text-anchor="middle">n</text>
  </g>
  
  <!-- Legend -->
  <rect x="40" y="150" width="10" height="10" fill="#b3d9ff" stroke="#000000" stroke-width="0.5"/>
  <text x="55" y="157.5" font-family="Arial" font-size="6" fill="#000000">Training Set</text>
  <rect x="40" y="165" width="10" height="10" fill="#f5e6c9" stroke="#000000" stroke-width="0.5"/>
  <text x="55" y="172.5" font-family="Arial" font-size="6" fill="#000000">Validation Set (1 observation)</text>
</svg>
```
:::

## Example: Leave-One-Out Cross Validation (LOOCV) (1/4) {.smaller}

::: columns
::: {.column width="50%"}
### Fit a linear model using glm()

```{r}
#| code-fold: show

auto_mpg_glm <- glm(mpg ~ horsepower, data = auto_mpg_9_final)
summary(auto_mpg_glm)
```
:::

::: {.column width="50%"}
### Fit a linear model using lm()

```{r}
#| code-fold: show

auto_mpg_lm <- lm(mpg ~ horsepower, data = auto_mpg_9_final)
summary(auto_mpg_lm)
```
:::
:::

WE perform linear regression using the `glm()` function rather than the `lm()` function because the former can be used together with `cv.glm()`. The `cv.glm()` function is part of the `boot` library (functions for bootstrapping).

## Example: Leave-One-Out Cross Validation (LOOCV) (2/4)

### Perform LOOCV using the `cv.glm()` function

```{r}
#| code-fold: show

library(boot)

auto_mpg_cv_glm <- cv.glm(auto_mpg_9_final, auto_mpg_glm, K = nrow(auto_mpg_9_final))
auto_mpg_cv_glm$delta
```

<br>

### LOOCV estimate for the test MSE

$$
CV_{(n)} = \frac{1}{n} \sum_{i=1}^{n} \text{MSE}_i
$$

## Example: Leave-One-Out Cross Validation (LOOCV) (3/4)

LOOCV is a general method and can be used with any kind of predictive modeling.

<br>

### Advantages and Disadvantages of LOOCV

**Advantages**

-   Less bias as the method using n-1 observations to fit the model.\
-   No randomness in the data split (unlike the validation approach).

**Disadvantages**

-   Computationally expensive for large datasets.

## Example: Leave-One-Out Cross Validation (LOOCV) (4/4)

### Perform LOOCV over a range of polynomial degrees

```{r}
#| code-fold: show
#| code-line-numbers: "|2-17|3|6|7|11|13|"

# Create a function to perform LOOCV over a range of polynomial degrees
auto_mpg_cv_glm_poly_loocv <- function(max_degree = 3) {
  results <- tibble()
  
  for (degree in 1:max_degree) {
    formula <- as.formula(paste("mpg ~ poly(horsepower, degree =", degree, ")"))
    auto_mpg_glm <- glm(formula, data = auto_mpg_9_final)
    
    cv_results <- cv.glm(auto_mpg_9_final, auto_mpg_glm, K = nrow(auto_mpg_9_final))
    
    mse <- cv_results$delta[1]
    
    results <- bind_rows(results, tibble(Degree = degree, MSE = mse))
  }
  
  return(results)
}

# Example usage:
result_df <- auto_mpg_cv_glm_poly_loocv(10)
result_df |> gt::gt()  
```

## k-Fold Cross-Validation

## 

::: r-stack
```{=html}
<svg xmlns="http://www.w3.org/2000/svg" width="600" height="600" viewBox="0 0 225 225">
  <!-- Title -->
  <text x="112.5" y="12.5" font-family="Arial" font-size="9" fill="#000000" text-anchor="middle" font-weight="bold">K-Fold Cross Validation</text>

  <!-- All Data -->
  <rect x="62.5" y="20" width="100" height="15" fill="#b3d9ff" stroke="#000000" stroke-width="0.5"/>
  <text x="112.5" y="30" font-family="Arial" font-size="6" fill="#000000" text-anchor="middle">All Data (n observations)</text>
  
  <!-- Arrow -->
  <line x1="112.5" y1="37.5" x2="112.5" y2="47.5" stroke="#000000" stroke-width="1"/>
  <polygon points="110,47.5 112.5,52.5 115,47.5" fill="#000000"/>
  
  <!-- K-Fold Iterations -->
  <g transform="translate(37.5,55)">
    <!-- Iteration 1 -->
    <rect x="25" y="0" width="100" height="12.5" fill="#b3d9ff" stroke="#000000" stroke-width="0.5"/>
    <rect x="25" y="0" width="20" height="12.5" fill="#f5e6c9" stroke="#000000" stroke-width="0.5"/>
    <text x="35" y="8.5" font-family="Arial" font-size="6" fill="#000000" text-anchor="middle">1</text>
    <text x="120" y="8.5" font-family="Arial" font-size="6" fill="#000000" text-anchor="end">Iteration 1</text>
    
    <!-- Iteration 2 -->
    <rect x="25" y="15" width="100" height="12.5" fill="#b3d9ff" stroke="#000000" stroke-width="0.5"/>
    <rect x="45" y="15" width="20" height="12.5" fill="#f5e6c9" stroke="#000000" stroke-width="0.5"/>
    <text x="55" y="23.5" font-family="Arial" font-size="6" fill="#000000" text-anchor="middle">2</text>
    <text x="120" y="23.5" font-family="Arial" font-size="6" fill="#000000" text-anchor="end">Iteration 2</text>
    
    <!-- Iteration 3 -->
    <rect x="25" y="30" width="100" height="12.5" fill="#b3d9ff" stroke="#000000" stroke-width="0.5"/>
    <rect x="65" y="30" width="20" height="12.5" fill="#f5e6c9" stroke="#000000" stroke-width="0.5"/>
    <text x="75" y="38.5" font-family="Arial" font-size="6" fill="#000000" text-anchor="middle">3</text>
    <text x="120" y="38.5" font-family="Arial" font-size="6" fill="#000000" text-anchor="end">Iteration 3</text>
    
    <!-- Ellipsis to indicate continuation -->
    <text x="75" y="52.5" font-family="Arial" font-size="10" fill="#000000" text-anchor="middle">...</text>
    
    <!-- Last Iteration -->
    <rect x="25" y="62.5" width="100" height="12.5" fill="#b3d9ff" stroke="#000000" stroke-width="0.5"/>
    <rect x="105" y="62.5" width="20" height="12.5" fill="#f5e6c9" stroke="#000000" stroke-width="0.5"/>
    <text x="115" y="71" font-family="Arial" font-size="6" fill="#000000" text-anchor="middle">k</text>
    <text x="55" y="71" font-family="Arial" font-size="6" fill="#000000" text-anchor="end">Iteration k</text>
  </g>
  
  <!-- Legend -->
  <rect x="40" y="150" width="10" height="10" fill="#b3d9ff" stroke="#000000" stroke-width="0.5"/>
  <text x="55" y="157.5" font-family="Arial" font-size="6" fill="#000000">Training Set</text>
  <rect x="40" y="165" width="10" height="10" fill="#f5e6c9" stroke="#000000" stroke-width="0.5"/>
  <text x="55" y="172.5" font-family="Arial" font-size="6" fill="#000000">Validation Set (1/k of data)</text>
</svg>
```
:::

## Example: k-Fold Cross-Validation (1/1) {.smaller}

### Perform k-Fold Cross-Validation using the `cv.glm()` function

```{r}
#| code-fold: show

auto_mpg_cv_glm_kfold <- cv.glm(auto_mpg_9_final, auto_mpg_glm, K = 10)
auto_mpg_cv_glm_kfold$delta
```

<br>

### k-Fold estimate for the test MSE

$$
CV_{(k)} = \frac{1}{k} \sum_{i=1}^{k} \text{MSE}_i
$$

## Example: k-Fold Cross-Validation (2/2)

### Perform k-Fold Cross-Validation over a range of polynomial degrees

```{r}
# Create a function to perform LOOCV over a range of polynomial degrees
auto_mpg_cv_glm_poly_kfold <- function(max_degree = 3, K = 10) {
  results <- tibble()
  
  for (degree in 1:max_degree) {
    formula <- as.formula(paste("mpg ~ poly(horsepower, degree =", degree, ")"))
    auto_mpg_glm <- glm(formula, data = auto_mpg_9_final)
    
    cv_results <- cv.glm(auto_mpg_9_final, auto_mpg_glm, K = K)
    
    mse <- cv_results$delta[1]
    
    results <- bind_rows(results, tibble(Degree = degree, MSE = mse))
  }
  
  return(results)
}

# Example usage:
result_df <- auto_mpg_cv_glm_poly_kfold(10, 10)
result_df |> gt::gt()  
```

## Example: k-Fold Cross-Validation (3/3)

### Compare the time for the LOOCV and k-fold CV methods

```{r}
#| code-fold: show

# Time taken for LOOCV
system.time(auto_mpg_cv_glm_poly_loocv(10))
```

<br>

```{r}
#| code-fold: show

# Time taken for k-Fold CV
system.time(auto_mpg_cv_glm_poly_kfold(10,10))
```

## Bias-Variance Trade-Off {.smaller}

::: columns
::: {.column width="50%"}
The bias-variance trade-off is a fundamental concept in machine learning that deals with the balance between a model's ability to fit the training data (low bias) and its ability to generalize to new, unseen data (low variance).

Cross-validation helps us understand and manage this trade-off.

In cross-validation:

1.  We split our data into training and validation sets multiple times.\
2.  For each split, we train our model on the training set and evaluate it on the validation set.\
3.  We average the performance across all splits.
:::

::: {.column width="50%"}
::: fragment
Bias:

-   Low bias: Model fits training data well but may over fit.\
-   High bias: Model is too simple and underfits both training and validation data.

Variance:

-   Low variance: Model performs consistently across different validation sets.\
-   High variance: Model's performance varies significantly between validation sets.

As we adjust model complexity:

1.  Simple models:
    -   High bias, low variance\
    -   Underfits training data\
    -   Similar (but poor) performance across validation sets
2.  Complex models:
    -   Low bias, high variance\
    -   Fits training data well\
    -   Performance varies widely across validation sets
:::
:::
:::

::: fragment
::: transparent
The goal is to find the sweet spot where the model complexity balances bias and variance, minimizing overall error.

Cross-validation helps us identify this balance by:

1.  Revealing over fitting: If performance is much better on training than validation sets\
2.  Showing under fitting: If performance is poor on both training and validation sets\
3.  Indicating generalization: Consistent performance across validation sets suggests good balance
:::
:::

# The Bootstrap {.theme-section}

## Introduction to the Bootstrap

## Bootstrap Visulaization  

<iframe src="bootstrap_visualization.html" width="620" height="520" style="border:none;"></iframe>


## Example 1: The Bootstrap (1/2)  

### Let's use the bootstrap method to estimate the mean and standard error for a uniform distribution 

```{r}
#| code-fold: true
#| fig-width: 6
#| out-width: 600px

# Generate data from a uniform distribution
set.seed(42)

n <- 1000
data <- runif(n, min = 0, max = 10)

# Histogram of the data 
data |> 
  as_tibble_col(column_name = "data") |> 
  ggplot() +
  geom_histogram(aes(x = data), binwidth = 0.5, fill = "skyblue", color = "black", boundary = 0) +
  labs(
    title = "Histogram of the Uniform Distribution",
    x = "Data",
    y = "Density"
  ) +
  theme_light()
```

## Example 1: The Bootstrap (2/2)


```{r}
#| code-fold: true
#| fig-width: 6
#| out-width: 600px

# Bootstrap function
bootstrap_mean <- function(data, B = 1000) {
  n <- length(data) # data is a vector
  
  mean_values <- replicate(B, {
    indices_sample <- sample(1:n, replace = TRUE)
    mean(data[indices_sample])
  })
  
  return(mean_values)
}

# Use the bootstrap function
mean_values <- bootstrap_mean(data, B = 1000)

mean_values |> 
  as_tibble_col(column_name = "mean_bootstrap") |> 
  ggplot() +
  geom_histogram(aes(x = mean_bootstrap, y = after_stat(density)), bins = 30, fill = "skyblue", color = "black") +
  geom_function(fun = dnorm, args = list(mean = mean(mean_values), sd = sd(mean_values)), color = "blue") +
  labs(
    title = "Bootstrap Distribution of the Mean of Uniform Distribution",
    x = "Mean",
    y = "Density"
  ) +
  theme_light()
```

## Example 2: The Bootstrap (1/5) 

According to the NIST book, the mean and standard deviation are given by the following

$$
\text{Mean} = \frac{A + B}{2}
$$

The calculated mean is `r round(mean(mean_values), 1)`.
Of course, the mean from the formula above is simply `r 5`.




## Example 2: The Bootstrap (2/5) {.smaller}  

Simulated Data of 100 pairs of returns.

:::: {.columns}

::: {.column width="30%"}

```{r}
portfolio <- ISLR2::Portfolio
portfolio
```


:::  

::: {.column width="70%"}  

```{r}
p1 <- portfolio |> 
  ggplot() +
  geom_line(aes(x = seq_along(X), y = X)) +
  geom_line(aes(x = seq_along(Y), y = Y), color = "red") +
  labs(
    title = "Simulated Portfolio Data",
    x = "Sequence",
    y = "Change in Returns"
  ) +
  annotate("text", x = 85, y = -2.5, label = "Investment X", color = "black") +
  annotate("text", x = 85, y = -2.75, label = "Investment Y", color = "red") +
  theme_light()

p2 <- portfolio |> 
  ggplot() +
  geom_point(aes(x = X, y = Y)) +
  labs(
    title = "Scatterplot of Simulated Portfolio Data",
    x = "X",
    y = "Y"
  ) +
  theme_light()

p1 + p2
```

:::  

::::  

## Example 2: The Bootstrap (3/5)  

:::: {.columns}  

::: {.column width="60%"}  

> . . . we wish to invest a fixed sum of mony in two financial assests that yeild returns of $X$ and $Y$, respectively, where $X$ and $Y$ are random quantities. We will invest a fraction $\alpha$ of our money in $X$ and will invest the remaining $1-\alpha$ in $Y$ . . . we wish to choose $\alpha$ to minimize the total risk, or variance of our investment.

$$
\alpha = \frac{\sigma_Y^2 - \sigma_{XY}}{\sigma_X^2 + \sigma_Y^2 - 2\sigma_{XY}}
$$

where $\sigma_X^2$ is the variance of $X$, $\sigma_Y^2$ is the variance of $Y$, and $\sigma_{XY}$ is the covariance between $X$ and $Y$.

(We don't know these values, but can estimate them from the data, so in reality, we have $\hat{\alpha}$.)

:::  

::::  

## Example 2: The Bootstrap (4/5)  

### Using the Bootstrap to estimate the standard error of $\hat{\alpha}$

```{r}
#| code-fold: show

# Function to calculate alpha
calculate_alpha <- function(data, indices) {
  X <- data$X[indices]
  Y <- data$Y[indices]
  
  sigma_X <- var(X)
  sigma_Y <- var(Y)
  sigma_XY <- cov(X, Y)
  
  alpha <- (sigma_Y - sigma_XY) / (sigma_X + sigma_Y - 2 * sigma_XY)
  return(alpha)
}

# Bootstrap function
bootstrap_alpha <- function(data, B = 1000) {
  n <- nrow(data)
  indices <- 1:n
  
  alpha_values <- replicate(B, {
    indices_sample <- sample(indices, replace = TRUE)
    calculate_alpha(data, indices_sample)
  })
  
  return(alpha_values)
}

# Example usage
set.seed(42)
alpha_values <- bootstrap_alpha(portfolio, B = 1000)
mean(alpha_values)
```

```{r}
alpha_values |> 
  as_tibble_col(column_name = "alpha_bootstrap") |> 
  ggplot() +
  geom_histogram(aes(x = alpha_bootstrap, y = after_stat(density)),bins = 30, fill = "skyblue", color = "black") +
  geom_function(fun = dnorm, args = list(mean = mean(alpha_values), sd = sd(alpha_values)), color = "blue") +
  labs(
    title = "Bootstrap Distribution of Alpha",
    x = "Alpha",
    y = "Density"
  ) +
  theme_light()
```

## Example 2: The Bootstrap (5/5) 

### Estimate the uncertainty in $\hat{\alpha}$

```{r}
#| code-fold: show

alpha_values |> 
  as_tibble_col(column_name = "alpha_bootstrap") |> 
  summarise(
    Mean = mean(alpha_bootstrap),
    SD = sd(alpha_bootstrap),
    SE = sd(alpha_bootstrap) / sqrt(length(alpha_bootstrap)),
    UCI_95 = Mean + SE*qt(c(0.975), length(alpha_bootstrap) - 1),
    LCI_95 = Mean - SE*qt(0.975, length(alpha_values) - 1)
  )
```




#  {.theme-section visibility="uncounted"}

<h1>End of Module 8</h1>

## References

::: {#refs}
:::

## Sandbox {visibility="hidden"}


```{r}
test_tibble <- tibble(
  x = 1:10,
  y = (1:10)^2
)

test_tibble
```

```{r}
# Load required library
library(tidyverse)

# Your original data frame
test_tibble <- tibble(
  x = 1:10,
  y = (1:10)^2
)

# Function to stack the data frame n times
stack_dataframe <- function(df, n_repeats) {
  map(1:n_repeats, \(repeat_num) {
    df |> 
      mutate(repeat_number = repeat_num)
    }) |> 
    list_rbind()
}

# Create the new stacked data frame (e.g., repeating 5 times)
stacked_df <- stack_dataframe(test_tibble, 5)

# View the result
print(stacked_df)
```


