---
title: "Statistical Learning (Machine Learning), <br>Linear Regression"
subtitle: "Module 06"
author: "Ray J. Hoobler"
bibliography: references.bib
execute:
  echo: true
  cache: false # requires a specific page
title-slide-attributes:
  data-background-color: "#1178c9"
format: 
  revealjs:
    df-print: paged
    toc: true
    toc-depth: 1
    toc-title: "Table of Contents"
    embed-resources: true
    link-external-icon: true
    link-external-newwindow: true
    incremental: false
    smaller: false
    scrollable: true
    transition: fade
    code-fold: true
    code-tools: true
    show-slide-number: all
    slide-number: c/t
    footer: "Applied Statistical Techniques"
    theme: [simple, mysimple.scss]
---  

## Libraries 

```{r}
#| code-fold: show 
library(tidyverse)  
library(ggthemes)
library(patchwork)
library(broom)
library(ggtext)
library(latex2exp)
```


# Statistical Learning {.theme-section}

## What is Statistical Learning? (1/9)

:::: {.columns}  

::: {.column width=50%}  

[An Introduction to Statistical Learning](https://www.statlearning.com/)  

![](https://m.media-amazon.com/images/I/61XlRprVVML._SL1246_.jpg){width=50%}

:::  

::: {.column width=50%}  

>**_Statistical learning_** refers to a vast set of tools for **_understanding data_**. These tools can be classified as **_supervised_** or **_unsupervised_**. Broadly speaking, supervised statistical learning involves building a statistical model for predicting, or estimating, an output based on one or more inputs. <br>
*- Introduction, ISLR*

:::  

::::  

## What is Statistical Learning? (2/9)

$$
Y = f(X_1, X_2, \ldots, X_p) + \epsilon
$$

$f$ is a fixed but unknown function of $X_1, X_2, \ldots, X_p$, and $\epsilon$ is a random error term, which is independent of $X$ and has a mean of zero.

We want to predict a value Y based on the values of X assuming a functional relationship between Y and X.


```{r}
# sales in thousands of units
# market budgets in thousands of dollars
advertising <- read_csv("datasets/Advertising.csv", col_names = TRUE, 
                        col_select = c(2:5), show_col_types = FALSE)
```


```{r}
#| code-fold: true
#| message: false

p1 <- advertising |> 
  ggplot(aes(x = TV, y = Sales)) +
  geom_point(color = "red", shape = "circle open", stroke = 1) +
  geom_smooth(method = "lm", color = "blue", se = FALSE, linewidth = 2) +
  labs(x = "TV", y = "Sales") +
  scale_y_continuous(breaks = seq(0, 30, 5)) +
  theme_light() 

p2 <- advertising |> 
  ggplot(aes(x = Radio, y = Sales)) +
  geom_point(color = "red", shape = "circle open", stroke = 1) +
  geom_smooth(method = "lm", color = "blue", se = FALSE, linewidth = 2) +
  labs(x = "Radio", y = "Sales") +
  scale_y_continuous(breaks = seq(0, 30, 5)) +
  theme_light()

p3 <- advertising |>
  ggplot(aes(x = Newspaper, y = Sales)) +
  geom_point(color = "red", shape = "circle open", stroke = 1) +
  geom_smooth(method = "lm", color = "blue", se = FALSE, linewidth = 2) +
  labs(x = "Newspaper", y = "Sales") +
  scale_y_continuous(breaks = seq(0, 30, 5)) +
  theme_light()

p1 + p2 + p3
```

## What is Statistical Learning? (3/9)  

:::: {.columns}

::: {.column width=50%}  

```{r}
#| code-fold: true
#| fig-width: 4
#| fig-height: 4
#| out-height: 400px
#| out-width: 400px

income_1 <- read_csv("datasets/Income1.csv", col_names = TRUE, col_select = c(2:3), col_types = "dd")

# income_1

income_1 |> 
  ggplot(aes(x = Education, y = Income)) +
  geom_point(color = "red", shape = "circle open", stroke = 1) +
  geom_smooth(method = "lm", color = "blue", se = FALSE) +
  geom_smooth(method = "loess", color = "orange", se = FALSE) +
  labs(x = "Years of Education", y = "Income") +
  scale_y_continuous(breaks = seq(0, 100, 10)) +
  scale_x_continuous(limits = c(10, 22), breaks = seq(10, 22, 2)) +
  theme_light()
```

:::  

::: {.column width=50%}  

$$
\textbf{Income} = \beta_0 + \beta_1 \times \textbf{Education} \\
\, \\ 
\textbf{Income} = \frac{a}{1 + \exp(-b(\textbf{Education} - c))} + d
$$

<br> 

::: {.fragment}  
How do we choose our model?
:::  

:::  

::::  

## What is Statistical Learning? (4/9)  

```{r}
#| code-fold: true
#| fig-width: 4
#| fig-height: 4
#| fig-align: center
#| out-width: 500px
#| out-height: 500px

income_function <- function(x, a, b, c, d) {
 d + a / (1 + exp(-b * (x - c)))
}

a <- 60
b <- 0.5
c <- 16
d <- 20

fit <- nls(Income ~ income_function(Education, a, b, c, d), data = income_1,
           start = list(a = a, b = b, c = c, d = d))

coef_fit <- coef(fit)
# coef_fit

income_1_augment <- augment(fit, income_1)

p1 <- income_1_augment |>
  ggplot(aes(x = Education, y = Income)) +
  geom_segment(aes(xend = Education, yend = .fitted), color = "black", linetype = "solid",  linewidth = 0.5) +
  geom_point(color = "red", shape = "circle") +
  geom_function(fun = function(x) income_function(x, 
                                                  a = coef_fit["a"],
                                                  b = coef_fit["b"],
                                                  c = coef_fit["c"],
                                                  d = coef_fit["d"]), 
                color = "blue", linewidth = 1) +
  labs(x = "Years of Education", y = "Income") +
  scale_y_continuous(breaks = seq(0, 100, 10)) +
  scale_x_continuous(limits = c(10, 22), breaks = seq(10, 22, 2)) +
  theme_light()

p2 <- income_1_augment |>
  ggplot(aes(x = Education, y = .resid)) +
  geom_hline(yintercept = 0, color = "black", linetype = "dashed") +
  geom_point(color = "red", shape = "circle") +
  labs(x = "Years of Education", y = "Residuals") +
  scale_y_continuous(limits = c(-10, 10), breaks = seq(-10, 10, 5)) +
  scale_x_continuous(limits = c(10, 22), breaks = seq(10, 22, 2)) +
  theme_light()

p1 / p2 +
  plot_layout(heights = c(3, 1), axes = "collect")
  
```

## What is Statistical Learning? (5/9)

```{r}
income_2 <- read_csv("datasets/Income2.csv", col_names = TRUE, col_select = c(2:4), 
                     col_types = "dd")

income_2
```


## What is Statistical Learning? (6/9)  

```{r}
#| echo: false
#| fig-width: 6
#| out-width: 600px

# create a surface plot of the income_2 data
income_2 |> 
  plotly::plot_ly(x = ~Education, y = ~Seniority, z = ~Income, type = "scatter3d",mode = "markers")

```

## What is Statistical Learning? (7/9)  

```{r}
#| fig-width: 6
#| fig-height: 4
#| fig-align: center
#| out-width: 600px
#| out-height: 400px 

income_2 |> 
  ggplot(aes(x = Seniority, y = Income)) +
  geom_smooth(method = "lm", se = FALSE, formula = y ~ x) +
  geom_point(aes(color = Education))
```

## What is Statistical Learning? (8/9) {.smaller}

:::: {.columns}  

::: {.column width=50%}  

```{r}
#| code-fold: true
#| fig-width: 4
#| fig-height: 4
#| out-width: 500px
#| out-height: 500px


# fit the income data to a function that is sigmoid shape in Education and linear in Seniority

income_function_2 <- function(x, y, a, b, c, d, e) {
  d + a / (1 + exp(-b * (x - c))) + e * y
}

a <- 60
b <- 0.5
c <- 16
d <- 20
e <- 0.5

fit_2 <- nls(Income ~ income_function_2(Education, Seniority, a, b, c, d, e), data = income_2,
           start = list(a = a, b = b, c = c, d = d, e = e))

# summary(fit_2)
# coef_fit_2 <- coef(fit_2)
# coef_fit_2

income_2_augment <- augment(fit_2, income_2)

## plot predicted values against actual values

p1 <- income_2_augment |>
  ggplot(aes(x = Income, y = .fitted)) +
  geom_point(color = "red", shape = "circle") +
  geom_abline(intercept = 0, slope = 1, color = "blue", linetype = "dashed") +
  labs(x = "Actual Income", y = "Predicted Income") +
  scale_x_continuous(breaks = seq(0, 100, 10)) +
  scale_y_continuous(breaks = seq(0, 100, 10)) +
  theme_light()

p2 <- income_2_augment |>
  ggplot(aes(x = Income, y = .resid)) +
  geom_hline(yintercept = 0, color = "black", linetype = "dashed") +
  geom_point(color = "red", shape = "circle") +
  labs(x = "Actual Income", y = "Residuals") +
  scale_x_continuous(breaks = seq(0, 100, 10)) +
  scale_y_continuous(limits = c(-15, 15), breaks = seq(-10, 10, 5)) +
  theme_light()

p1 / p2 +
  plot_layout(heights = c(3, 1), axes = "collect")
```

:::

::: {.column width=50%}  

<br>  

$$
\textbf{Income} = \frac{a}{1 + \exp(-b(\textbf{Education} - c))} + d + e (\textbf{Seniority})
$$

:::  

::::  

## What is Statistical Learning? (9/9)  

```{r}
#| fig-width: 6
#| out-width: 600px


predict_income <- function(education, seniority) {
  coef <- coef(fit_2)
  coef['d'] + coef['a'] / (1 + exp(-coef['b'] * (education - coef['c']))) + coef['e'] * seniority
}

# Create sequences and grid
education_seq <- seq(min(income_2$Education), max(income_2$Education), length.out = 100)
seniority_seq <- seq(min(income_2$Seniority), max(income_2$Seniority), length.out = 100)
grid <- expand.grid(Education = education_seq, Seniority = seniority_seq)
grid$Income <- predict_income(grid$Education, grid$Seniority)


# 3. 3D scatter plot
p3 <- plotly::plot_ly(width = 500, height = 500) %>%
  plotly::add_markers(data = grid, x = ~Education, y = ~Seniority, z = ~Income, 
              marker = list(size = 3, color = ~Income, colorscale = "Viridis"),
              showlegend = FALSE) %>%
  plotly::add_markers(data = income_2, x = ~Education, y = ~Seniority, z = ~Income,
              marker = list(size = 5, color = "red", symbol = "circle"),
              showlegend = FALSE) %>%
  plotly::layout(scene = list(
    xaxis = list(title = "Education"),
    yaxis = list(title = "Seniority"),
    zaxis = list(title = "Income")
  ),
  title = "3D Income Scatter")


# 4. Static 3D surface plot
# png("persp_plot.png", width = 800, height = 600)
# z_matrix <- matrix(grid$Income, nrow = length(education_seq), ncol = length(seniority_seq))
# persp(education_seq, seniority_seq, z_matrix, 
#       theta = 30, phi = 30, expand = 0.5, col = "lightblue",
#       xlab = "Education", ylab = "Seniority", zlab = "Income",
#       main = "3D Surface: Income vs Education and Seniority")
# points <- trans3d(income_2$Education, income_2$Seniority, income_2$Income, 
#                   pmat = persp(education_seq, seniority_seq, z_matrix, theta = 30, phi = 30, expand = 0.5, col = "lightblue"))
# points(points, col = "red", pch = 16)
# dev.off()

# Display 3D scatter plot
# The static 3D surface plot is saved as "persp_plot.png"
p3
```

## Assessing Model Accuracy: Regression (1/6)  

:::: {.columns}  

::: {.column width=60%}

$$
MSE = \frac{1}{n} \sum_{i=1}^{n} \left (y_i - \hat{f}(x_i) \right )^2
$$

Where $y_i$ is the true value of the response for observation $i$, and $\hat{f}(x_i)$ is the predicted value.

:::  

::::  

## Assessing Model Accuracy: Regression (2/6)  

### Create a dummy dataset with 100 observations that follow a polynomial function  

```{r}
#| code-fold: true
#| fig-width: 4
#| fig-height: 4
#| fig-align: center  
#| out-width: 400px
#| out-height: 400px
#| message: false

set.seed(142)
n <- 100
x <- runif(n, -2, 2)
y <- x^4 + rnorm(n, 0, 2)

poly_data <- tibble(x = x, y = y)

p1 <- poly_data |> 
  ggplot(aes(x = x, y = y)) +
  geom_point(color = "red", shape = "circle open", stroke = 1) +
  labs(x = "x", y = "y") +
  theme_light()

p1
```

## Assessing Model Accuracy: Regression (3/6)  

### Split the data into training and testing sets  



```{r}
#| code-fold: true
#| fig-width: 5
#| fig-height: 4
#| fig-align: center
#| out-width: 500px
#| out-height: 400px
#| message: false

set.seed(142)
train_index <- sample(1:n, n * 0.8)
# train_data <- poly_data[train_index, ]
# test_data <- poly_data[-train_index, ]

poly_data_sample <- poly_data |> 
  mutate(
    sample = ifelse(row_number() %in% train_index, "Train", "Test")
  )

# poly_data_sample

p1 <- poly_data_sample |> 
  ggplot(aes(x = x, y = y)) +
  geom_point(aes(color = sample, shape = sample), size = 2) +
  scale_color_colorblind() +
  labs(x = "x", y = "y") +
  theme_light()

p1

```

## Assessing Model Accuracy: Regression (4/6) {.smaller}  

:::: {.columns}  

::: {.column width=50%}  

### Fit a polynomial regression model to the training data  

```{r}
#| code-fold: true

poly_fit_train <- lm(y ~ poly(x, degree = 3), data = filter(poly_data_sample, sample == "Train"))
summary(poly_fit_train)
```

:::  

::: {.column width=50%}  

### Plot the polynomial regression model  

```{r}
#| code-fold: true
#| fig-width: 4
#| fig-height: 4
#| out-width: 400px
#| out-height: 400px
#| message: false

poly_data_augment <- augment(poly_fit_train, filter(poly_data_sample, sample == "Train"))

p1 <- poly_data_augment |> 
  ggplot(aes(x = x, y = y)) +
  geom_point(color = "red", shape = "circle open", stroke = 1) +
  geom_line(aes(y = .fitted), color = "blue", size = 1) +
  labs(x = "x", y = "y") +
  theme_light()

p1
```

:::  

::::  

## Assessing Model Accuracy: Regression (5/6)  

### Show the fit to the test data  

```{r}
#| code-fold: true
#| fig-width: 4
#| fig-height: 4
#| fig-align: center
#| out-width: 400px
#| out-height: 400px
#| message: false

p1 <- poly_data_augment |> 
  ggplot(aes(x = x, y = y)) +
  geom_point(data = filter(poly_data_sample, sample == "Test"), 
             color = "red", shape = "circle open", stroke = 1) +
  geom_line(aes(y = .fitted), color = "blue", size = 1) +
  labs(x = "x", y = "y") +
  theme_light()

p1
```

## Assessing Model Accuracy: Regression (6/6)  

### Calculate MSE for the training and test data as a function of polynomial degree  

```{r}
#| code-fold: true
#| fig-width: 6
#| fig-height: 4
#| fig-align: center
#| out-width: 600px
#| out-height: 400px
#| message: false

# Print summary of the data
# print(summary(poly_data_sample))

# Check the proportion of train/test split
# print(table(poly_data_sample$sample) / nrow(poly_data_sample))

# Initialize vectors to store MSE values
mse_train <- numeric(10)
mse_test <- numeric(10)

for (i in 1:10) {
  # Fit the model on training data
  poly_fit_train <- lm(y ~ poly(x, degree = i, raw = TRUE), 
                       data = filter(poly_data_sample, sample == "Train"))
  
  # Calculate MSE for training data
  train_augment <- augment(poly_fit_train, newdata = filter(poly_data_sample, sample == "Train"))
  mse_train[i] <- mean(train_augment$.resid^2)
  
  # Calculate MSE for test data
  test_augment <- augment(poly_fit_train, newdata = filter(poly_data_sample, sample == "Test"))
  mse_test[i] <- mean((test_augment$y - test_augment$.fitted)^2)
  
  # Print MSE for this degree
  # cat("Degree", i, "- Train MSE:", mse_train[i], "Test MSE:", mse_test[i], "\n")
}

# Create a data frame with results
mse_results <- data.frame(
  degree = 1:10,
  train_mse = mse_train,
  test_mse = mse_test
)

# Print results
# mse_results

# Plot MSE vs Polynomial Degree
ggplot(mse_results, aes(x = degree)) +
  geom_smooth(aes(y = train_mse, color = "Training"), se = FALSE, formula = y ~ x, method = "loess") +
  geom_smooth(aes(y = test_mse, color = "Test"), se = FALSE, formula = y ~ x, method = "loess") +
  geom_point(aes(y = train_mse, color = "Training")) +
  geom_point(aes(y = test_mse, color = "Test")) +
  scale_color_manual(values = c("Training" = "blue", "Test" = "red")) +
  labs(title = "MSE vs Polynomial Degree",
       x = "Polynomial Degree",
       y = "Mean Squared Error",
       color = "Dataset") +
  theme_light() +
  scale_y_continuous(expand = c(0, 0), limits = c(0, NA)) +
  scale_x_continuous(limits = c(1, 10), breaks = seq(0, 10, 1))
```

## Assessing Model Accuracy: Calssifiation (1/2)  


```{r}
make_moons <- function(n_samples = 100, noise = 0.05, random_state = NULL) {
  if (!is.null(random_state)) {
    set.seed(random_state)
  }
  
  n_samples_out <- ceiling(n_samples / 2)
  n_samples_in <- floor(n_samples / 2)
  
  outer_circ_x <- cos(seq(0, pi, length.out = n_samples_out))
  outer_circ_y <- sin(seq(0, pi, length.out = n_samples_out))
  inner_circ_x <- 1 - cos(seq(0, pi, length.out = n_samples_in))
  inner_circ_y <- 1 - sin(seq(0, pi, length.out = n_samples_in)) - 0.5
  
  X <- rbind(
    cbind(outer_circ_x, outer_circ_y),
    cbind(inner_circ_x, inner_circ_y)
  )
  
  y <- c(rep(0, n_samples_out), rep(1, n_samples_in))
  
  if (noise > 0) {
    X <- X + matrix(rnorm(n_samples * 2, sd = noise), ncol = 2)
  }
  
  return(list(X = X, y = y))
}
```

```{r}
#| fig-width: 5
#| fig-height: 4
#| fig-align: center
#| out-width: 500px
#| out-height: 400px 

# Generate moon-shaped data
set.seed(42)  # For reproducibility
result <- make_moons(n_samples = 200, noise = 0.3, random_state = 42)

# Extract features and labels
X <- result$X
y <- result$y

df <- data.frame(X, Class = as.factor(y))

ggplot(df, aes(x = outer_circ_x, y = outer_circ_y)) +
  geom_point(aes(color = Class, shape = Class), size = 2) +
  labs(
    x = "X",
    y = "Y") +
  scale_color_colorblind() +
  theme_light() 
```
## Assessing Model Accuracy: Classification (2/2)

### Fit the data to a KNN model 

**Accuracy**

```{r}
#| code-fold: true
#| fig-width: 6
#| out-width: 600px
#| message: false
#| warning: false

# Split the data into training and testing sets
train_index <- sample(1:nrow(df), nrow(df) * 0.8)
train_data <- df[train_index, ]
test_data <- df[-train_index, ]

# Fit the KNN model
knn_fit <- class::knn(train = train_data[, 1:2], test = test_data[, 1:2], cl = train_data$Class, k = 5)

# Calculate accuracy
accuracy <- mean(knn_fit == test_data$Class)
accuracy
```

<br>  
**Confusion Matrix**

```{r}
# Calculate confusion matrix
confusion_matrix <- table(Predicted = knn_fit, Actual = test_data$Class)
confusion_matrix
```

# Linear Regression {.theme-section}  

## Simple Linear Regression (1/9)

### Linear relationship 

::: {.fragment}

$$
Y \approx \beta_0 + \beta_1 X
$$

::: 

<br> 

::: {.fragment}  

For the Advertising data, we might have: 

$$
\textbf{Sales} \approx \beta_0 + \beta_1 \times \textbf{TV}
$$

Where $\beta_0$ is the intercept and $\beta_1$ is the slope. These are unknown constants.

:::  

::: {.fragment}  
Once we estimate the coefficients $\beta_0$ and $\beta_1$, we can make predictions.

$$
\hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 X
$$
:::  

## Simple Linear Regression (2/9)

:::: {.columns}  

::: {.column width=50%}  

### Review the **Advertising** data  

```{r}
#| code-fold: true
#| fig-width: 5
#| fig-height: 4
#| out-width: 500px
#| out-height: 400px

advertising_fit <- lm(Sales ~ TV, data = advertising)

advertising_augment <- augment(advertising_fit, advertising)

advertising_augment |> 
  ggplot(aes(TV, Sales)) + 
   geom_segment(aes(xend = TV, yend = .fitted), color = "black", linetype = "solid", linewidth = 0.25) +
  geom_point(color = "red", shape = "circle") +
  geom_smooth(formula = y ~ x, method = "lm", se = FALSE, color = "blue", size = 1) +
  scale_y_continuous(breaks = seq(0, 30, 5)) +
  theme_light()
```

:::  

::: {.column width=50%}  

### What is our goal?  

- Find coefficients for $\beta_0$ and $\beta_1$  
- Minimize the sum of squared residuals

$$
\text{RSS} = \sum_{i=1}^{n} \left (y_i - \hat{y}_i \right )^2
$$

:::  

::::  

## Simple Linear Regression (3/9) 

### Least Squares 

The least squares approach chooses $\hat{\beta}_0$ and $\hat{\beta}_1$ to minimize the RSS.

$$
RSS = \sum_{i=1}^{n} \left (y_i - (\hat{\beta_0} + \hat{\beta_1}x_i) \right )^2
$$
1. Take the partial derivative of the RSS with respect to $\beta_0$ and $\beta_1$  
2. Set the derivatives to zero  
3. Solve the resulting system of two equations with two unknowns, $\beta_0$ and $\beta_1$  

### Solution

$$
\hat{\beta}_1 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}
$$

$$
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}
$$

## Simple Linear Regression (4/9) {.smaller}  

### Linear model in R  

```{r}
#| code-fold: show

# Fit a simple linear regression model
advertising_TV_fit <- lm(Sales ~ TV, data = advertising)

# Print the summary of the model
summary(advertising_TV_fit)
```

### Extracting the coefficients  

```{r}
#| code-fold: show

# Extract the coefficients
coef(advertising_TV_fit)
coef(advertising_TV_fit)["TV"]
coef(advertising_TV_fit)["(Intercept)"]
```




## Simple Linear Regression (5/9)  

### Create a contour plot of the RSS with $\beta_1$ as a function of $\beta_0$  

```{r}
#| code-fold: true
#| fig-width: 5
#| fig-height: 4
#| fig-align: center
#| out-width: 500px
#| out-height: 400px
#| message: false
#| warning: false

# Create a grid of beta_0 and beta_1 values
beta_0 <- seq(5, 10, length.out = 100)
beta_1 <- seq(0.02, 0.08, length.out = 100)
grid <- expand.grid(beta_0 = beta_0, beta_1 = beta_1)

# Calculate RSS for each combination of beta_0 and beta_1
grid$RSS <- apply(grid, 1, function(row) {
  beta_0 <- row["beta_0"]
  beta_1 <- row["beta_1"]
  sum((advertising$Sales - (beta_0 + beta_1 * advertising$TV))^2)
})

# Create a contour plot
ggplot(grid, aes(x = beta_0, y = beta_1, z = RSS/1000)) +
  geom_contour_filled(breaks = seq(2.1, 3, 0.05)) +
  geom_point(aes(x = coef(advertising_fit)[1], y = coef(advertising_fit)[2]), color = "red", size = 2) +
  labs(title = "RSS Contour Plot",
       x = expression(beta[0]),
       y = expression(beta[1]),
       z = "RSS") +
  theme_light()
```

## Simple Linear Regression (6/9) {.smaller}  

:::: {.columns}  

::: {.column width=50%}  

### Variance of the coefficients 

$$
\text{Var}(\hat{\beta}_1) = \frac{\sigma^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2}
$$

$$
\text{Var}(\hat{\beta}_0) = \sigma^2 \left [ \frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \right ]
$$

where $\sigma^2$ is the variance of the error term $\epsilon$.

:::  
::: {.column width=50%}  

### Assessing the accuracy of the coefficient estimates 

```{r}
#| code-fold: true
#| fig-width: 6
#| out-width: 600px
#| message: false
#| warning: false

# Calculate the standard errors of the coefficient estimates
se <- sqrt(diag(vcov(advertising_fit)))

# Calculate the t-statistics
t_stat <- coef(advertising_fit) / se

# Calculate the p-values
p_value <- 2 * pt(abs(t_stat), df = nrow(advertising) - 2, lower.tail = FALSE)

# Create a data frame with the results
results <- data.frame(
  term = names(coef(advertising_fit)),
  estimate = coef(advertising_fit),
  std_error = se,
  t_statistic = t_stat,
  p_value = p_value
)

# Print the results
results

summary(advertising_fit)
```

:::  

::::  

## Simple Linear Regression (7/9)  

### Confidence intervals for the coefficients  

```{r}
#| code-fold: show
#| fig-width: 6
#| out-width: 600px
#| message: false
#| warning: false

# Calculate the confidence intervals for the coefficients from stats pacakge
conf_int <- confint(advertising_fit)

# Create a data frame with the results
results <- tibble(
  term = names(coef(advertising_fit)),
  estimate = coef(advertising_fit),
  lower = conf_int[, 1],
  upper = conf_int[, 2]
)

# Print the results
results
```

## Simple Linear Regression (8/9) {.smaller} 

:::: {.columns}  

::: {.column width=50%}  

### Plot the data <br>with confidence interval

```{r}
#| code-fold: true
#| fig-width: 6
#| out-width: 600px
#| message: false
#| warning: false

advertising_augment |> 
  ggplot(aes(x = TV, y = Sales)) +
  geom_point(color = "red", shape = "circle open", stroke = 1) +
  geom_smooth(formula = y ~ x, method = "lm", se = TRUE, color = "blue", size = 1) +
  labs(x = "TV", y = "Sales",
       title = "Scatter Plot of Advertising Data",
       subtitle = "se = TRUE") +
  scale_y_continuous(breaks = seq(0, 30, 5), limits = c(0, 30)) +
  theme_light()
```

:::  
::: {.column width=50%}  

::: {.fragment}  
### Plot the data with <br>confidence and prediction interval

```{r}
#| code-fold: true
#| fig-width: 6
#| out-width: 600px
#| message: false
#| warning: false

advertising_augment |> 
  ggplot(aes(x = TV, y = Sales)) +
  geom_point(color = "red", shape = "circle open", stroke = 1) +
  geom_smooth(formula = y ~ x, method = "lm", se = TRUE, color = "blue", size = 1) +
  geom_function(fun = function(x) predict(advertising_fit, newdata = data.frame(TV = x), interval = "predict")[, "lwr"],
                color = "black", linetype = "dashed") +
  geom_function(fun = function(x) predict(advertising_fit, newdata = data.frame(TV = x), interval = "predict")[, "upr"],
                color = "black", linetype = "dashed") +
  labs(x = "TV", y = "Sales",
       title = "Scatter Plot of Advertising Data",
       subtitle = "Demonstration of confidence and prediction intervals") +
  scale_y_continuous(breaks = seq(0, 30, 5), limits = c(0, 30)) +
  theme_light()
```
:::  
:::  

::::  

## Simple Linear Regression (9/9)  

### Assessing the accuracy of the model using the $R^2$ statistic  


$$
R^2 = \frac{\text{Explained Variance}}{\text{Total Variance}}
$$

<br> 

$$
R^2 = \frac{TSS - RSS}{TSS} \quad \text{or} \quad R^2 = 1 - \frac{RSS}{TSS}
$$

where TSS is the total sum of squares and RSS is the residual sum of squares.

$$
TSS = \sum_{i=1}^{n} (y_i - \bar{y})^2 \quad \text{and} \quad RSS = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$
<br>  

$R^2$ is the proportion of the variance in the response variable that is predictable from the predictor variable and takes values between 0 and 1.  

## Simple Linear Regression: Adjusted $R^2$

$$
\text{Adjusted } R^2 = 1 - \frac{RSS / (n - d -1)}{TSS / (n - 1)}
$$
where $n$ is the number of observations and $d$ is the number of predictors in the model.

## Multiple Linear Regression (1/7)  

### Advertising Data  

```{r}
#| code-fold: true

advertising
```

## Multiple Linear Regression (2/7)  

### Plot the Advertising Data  

```{r}
#| code-fold: true
#| fig-width: 8
#| fig-height: 4
#| fig-align: center
#| out-width: 800px
#| out-height: 400px

p1 <- advertising |> 
  ggplot() +
  geom_point(aes(x = TV, y = Sales), shape = "circle open", color = "red", size = 2, stroke = 1) +
  geom_smooth(aes(x = TV, y = Sales), formula = y~ x, method = "lm", se = FALSE, color = "blue", size = 1) +
  labs(x = "TV", y = "Sales") +
  scale_color_viridis_c() +
  theme_light()

p2 <- advertising |>
  ggplot() +
  geom_point(aes(x = Radio, y = Sales), shape = "circle open", color = "red", size = 2, stroke = 1) +
  geom_smooth(aes(x = Radio, y = Sales), formula = y~ x, method = "lm", se = FALSE, color = "blue", size = 1) +
  labs(x = "Radio", y = "Sales") +
  scale_color_viridis_c() +
  theme_light()

p3 <- advertising |>
  ggplot() +
  geom_point(aes(x = Newspaper, y = Sales), shape = "circle open", color = "red", size = 2, stroke = 1) +
  geom_smooth(aes(x = Newspaper, y = Sales), formula = y~ x, method = "lm", se = FALSE, color = "blue", size = 1) +
  labs(x = "Newspaper", y = "Sales") +
  scale_color_viridis_c() +
  theme_light()

p1 + p2 + p3
```

## Multiple Linear Regression (3/7) {.smaller}

### Fit a multiple linear regression model  

```{r}
#| code-fold: show

# Fit a multiple linear regression model
advertising_all_fit <- lm(Sales ~ TV + Radio + Newspaper, data = advertising)

# Print the summary of the model
summary(advertising_all_fit)
```

## Multiple Linear Regression (4/7)  

### Plot predicted values against actual values  

```{r}
#| code-fold: true
#| fig-width: 6
#| fig-height: 4
#| fig-align: center
#| out-width: 600px
#| out-height: 400px
#| message: false
#| warning: false

advertising_all_augment <- augment(advertising_all_fit, advertising)

p1 <- advertising_all_augment |> 
  ggplot(aes(x = Sales, y = .fitted)) +
  geom_point(color = "red", shape = "circle") +
  geom_abline(intercept = 0, slope = 1, color = "blue", linetype = "dashed") +
  labs(x = "Actual Sales", y = "Predicted Sales") +
  scale_x_continuous(breaks = seq(0, 30, 5)) +
  scale_y_continuous(breaks = seq(0, 30, 5)) +
  theme_light()

p2 <- advertising_all_augment |>
  ggplot(aes(x = Sales, y = .resid)) +
  geom_hline(yintercept = 0, color = "black", linetype = "dashed") +
  geom_point(color = "red", shape = "circle") +
  labs(x = "Actual Sales", y = "Residuals") +
  scale_x_continuous(breaks = seq(0, 30, 5)) +
  scale_y_continuous(limits = c(-10, 10), breaks = seq(-10, 10, 5)) +
  theme_light()

p1 / p2 +
  plot_layout(heights = c(3, 1), axes = "collect")
```

## Multiple Linear Regression (5/7)

### Interaction between features 

We can consider the interaction between features in the model. Mathematically, this would be equivalent to adding a new feature that is the product of two (or more) features. 

Fore example, we can create a new column in the dataset that is the product of the TV and Radio columns, TV * Radio; TV and Newspaper, TV * Newspaper; and Radio and Newspaper, Radio * Newspaper.  

We can also consider the product of all three columns, TV * Radio * Newspaper.
<br>

::: {.fragment}  

$$
\begin{multline}
\text{Sales} \approx \beta_0 + \beta_1 \times \text{TV} + \beta_2 \times \text{Radio} + \beta_3 \times \text{Newspaper} \\+ \beta_4 \times \text{TV} \times \text{Radio} + \beta_5 \times \text{TV} \times \text{Newspaper} + \beta_6 \times \text{Radio} \times \text{Newspaper} \\+ \beta_7 \times \text{TV} \times \text{Radio} \times \text{Newspaper}
\end{multline}
$$

:::  

## Multiple Linear Regression (6/7) {.smaller}  

### Revisit the Sales vs all markets

```{r}
#| code-fold: show

# Fit a multiple linear regression model and include a TV^2 term

advertising_crossterms_fit <- lm(Sales ~ TV*Radio*Newspaper, data = advertising)

# Print the summary of the model
summary(advertising_crossterms_fit)
```

## Multiple Linear Regression (7/7)  

### Plot of the Sales vs all markets  

```{r}
#| code-fold: true
#| fig-width: 5
#| fig-height: 4
#| fig-align: center
#| out-width: 500px
#| out-height: 400px
#| message: false
#| warning: false

advertising_crossterms_augment <- augment(advertising_crossterms_fit, advertising)

p1 <- advertising_crossterms_augment |> 
  ggplot(aes(x = Sales, y = .fitted)) +
  geom_point(color = "red", shape = "circle") +
  geom_abline(intercept = 0, slope = 1, color = "blue", linetype = "dashed") +
  labs(x = "Actual Sales", y = "Predicted Sales") +
  scale_x_continuous(breaks = seq(0, 30, 5)) +
  scale_y_continuous(breaks = seq(0, 30, 5)) +
  theme_light()

p2 <- advertising_crossterms_augment |>
  ggplot(aes(x = Sales, y = .resid)) +
  geom_hline(yintercept = 0, color = "black", linetype = "dashed") +
  geom_point(color = "red", shape = "circle") +
  labs(x = "Actual Sales", y = "Residuals") +
  scale_x_continuous(breaks = seq(0, 30, 5)) +
  scale_y_continuous(limits = c(-10, 10), breaks = seq(-10, 10, 5)) +
  theme_light()

p1 / p2 +
  plot_layout(heights = c(3, 1), axes = "collect")
```



## Assignment: Linear Regression (1/5) {.smaller}  

### Concrete Compressive Strength  

[UCI Machine Learning Repository](https://doi.org/10.24432/C5PK67)

Using the `Concrete_Data.csv` dataset, perform a linear regression analysis to predict the compressive strength of concrete using one or more of the other features in the dataset.

What was the best Adjusted $R^2$ value you achieved?

```{r}
#| code-fold: show

# New names for columns
concrete_names <- c("Cement", "Blast_Furnace_Slag", "Fly_Ash", "Water", "Superplasticizer", "Coarse_Aggregate", "Fine_Aggregate", "Age", "Compressive_Strength")

# Load the dataset
library(readxl)
concrete_data <- read_xls("datasets/concrete+compressive+strength/Concrete_Data.xls",
                          col_names = concrete_names,
                          skip = 1)

concrete_data
```

## Assignment: Linear Regression (2/5)

### EDA of dataset using ggpairs 

```{r}
#| code-fold: true
#| fig-width: 6
#| fig-height: 6
#| fig-align: center
#| out-width: 600px
#| out-height: 600px
#| message: false
#| warning: false

library(GGally)

concrete_data |> 
  ggpairs(lower = list(continuous = wrap("points", alpha = 1/10)))
```

## Assignment: Linear Regression (3/5) {.smaller}

### Fit a linear regression model 

```{r}
#| code-fold: show 

# Fit a linear regression model
concrete_fit_a <- lm(Compressive_Strength ~ Cement + Superplasticizer, data = concrete_data)

summary(concrete_fit_a)
```

## Assignment: Linear Regression (4/5) {.smaller}

### Fit a linear regression model to all the features

```{r}
#| code-fold: show

# Fit a linear regression model
concrete_fit_b <- lm(Compressive_Strength ~ ., data = concrete_data)

summary(concrete_fit_b)
```

## Assignment: Linear Regression (5/5) 

### Compare the models by plotting predicted vs actual values

```{r}
#| code-fold: true
#| fig-width: 8
#| fig-height: 4
#| fig-align: center
#| out-width: 800px
#| out-height: 400px
#| message: false
#| warning: false

concrete_augment_a <- augment(concrete_fit_a, concrete_data)
concrete_augment_b <- augment(concrete_fit_b, concrete_data)

p1 <- concrete_augment_a |> 
  ggplot(aes(x = Compressive_Strength, y = .fitted)) +
  geom_point(color = "red", shape = "circle", alpha = 1/5) +
  geom_abline(intercept = 0, slope = 1, color = "blue", linetype = "dashed") +
  labs(x = "Actual Compressive Strength", y = "Predicted Compressive Strength") +
  scale_x_continuous(breaks = seq(0, 100, 10)) +
  scale_y_continuous(breaks = seq(0, 100, 10), limits = c(0, 80)) +
  labs(title = "Cement and Superplasticizer") +
  theme_light()

p2 <- concrete_augment_b |>
  ggplot(aes(x = Compressive_Strength, y = .fitted)) +
  geom_point(color = "red", shape = "circle", alpha = 1/5) +
  geom_abline(intercept = 0, slope = 1, color = "blue", linetype = "dashed") +
  labs(x = "Actual Compressive Strength", y = "Predicted Compressive Strength") +
  scale_x_continuous(breaks = seq(0, 100, 10)) +
  scale_y_continuous(breaks = seq(0, 100, 10), limits = c(0, 80)) +
  labs(title = "All Features") +
  theme_light()

p1 + p2 +
  plot_layout(axes = "collect")
```



# {.theme-section visibility=uncounted}  

<h1>End of Module 6</h1>  

## References 

::: {#refs}
:::  